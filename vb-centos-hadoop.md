# Virtual Box安装CentOS搭建Hadoop集群

## Virtual Box安装CentOS

| 工具                | 版本                             |
| ------------------- | -------------------------------- |
| cenos               | CentOS-6.8-x86_64-bin-DVD1.iso   |
| Virtual Box         | VirtualBox-6.1.4-136177-Win.exe  |
| Virtual Box增强工具 | VBoxGuestAdditions_6.1.0_RC1.iso |

### Virtual Box设置

内存分配1024M

硬盘选中“使用主机输入输出缓存”和“固态驱动器”

网卡使用双网卡，第一个“NAT”，第二个“Host-Only”

### centos 分区

1. /boot 200M
2. /swap 2G
3. / 15G

### 增强工具安装

为了共享粘贴板和鼠标不被虚拟机独占等功能，需要安装增强工具，首先需要给centos安装必要的工具：

```shel
yum  -y  install kernel kernel-devel make gcc*
```

这里我还执行了yum update

挂载增强工具镜像，安装工具这里踩了很多坑，如果实在安装不成功建议跳过，反正以后大部分时间都是使用工具远程连接，这里我用默认的包安装不成功，去下了另外一个才安装成功，下载地址：http://download.virtualbox.org/virtualbox/

挂载过程：

<img src="https://irmp.github.io/images/image-20200305122549765.png" alt="image-20200305122549765" style="zoom:80%;" />

挂载之后桌面会弹出自动运行提示，或者去/media下也可以手动安装，安装成功之后注意开启共享粘贴板等功能。

<img src="https://irmp.github.io/images/image-20200305122840915.png" alt="image-20200305122840915" style="zoom:70%;" />

我出现过安装不报错，但是功能不可用的情况，重启虚拟机，重启宿主机之后又莫名好了，建议这里不要花费太多时间，可以后面有空再回来安装。

### 网络配置

1./etc/sysconfig/network-scripts/ifcfg-eth0 文件内容：

```shell
DEVICE=eth0
TYPE=Ethernet
UUID=c31b74b5-3bdc-4077-9a49-4fcc6c1a30c0
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=dhcp
DEFROUTE=yes
IPV4_FAILURE_FATAL=yes
IPV6INIT=no
NAME="System eth0"
HWADDR=08:00:27:18:b1:00
PEERDNS=yes
PEERROUTES=yes
LAST_CONNECT=1583335310
```

一般不用修改，注意ONBOOT是yes，BOOTPROTO是dhcp，该网卡用来连接外网

2./etc/sysconfig/network-scripts/ifcfg-eth1，没有这个文件就从ifcfg-eth0复制一份，内容：

```shell
DEVICE=eth1 #修改设备名称
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=static #修改为静态IP
DEFROUTE=yes
IPV4_FAILURE_FATAL=yes
IPV6INIT=no
NAME="System eth1" #修改网卡名称
IPADDR=192.168.56.11 #添加IP
HWADDR=08:00:27:8b:3b:25 #修改网卡MAC地址
PREFIX=24
UUID=9c92fad9-6ecb-3e6c-eb4d-8a47c6f50c04
LAST_CONNECT=1583377543
```

注意需要修改的几个地方

**IPADDR** 这个需要根据VBox设置去修改，设置-》主机网络管理器，设置成和IPv4地址同一个网段就可以了

<img src="https://irmp.github.io/images/image-20200305121458734.png" alt="image-20200305121458734" style="zoom: 80%;" />

**HWADDR**，网卡mac地址可以从虚拟机设置中获取：

<img src="https://irmp.github.io/images/image-20200305121724193.png" alt="image-20200305121724193" style="zoom:80%;" />

3.修改/etc/resolv.conf添加nameserver，没有就新建一个

```shell
# Generated by NetworkManager
domain Home
search Home
nameserver 192.168.1.1
```

nameserver 需要设置成自己宿主机的外网网关

4.修改主机名

/etc/sysconfig/network中修改HOSTNAME

```shell
NETWORKING=yes
HOSTNAME=hadoop01
NTPSERVERARGS=iburst
```

5.修改hosts文件，/etc/hosts中添加集群主机名和ip

修改后重启(reboot)，或者重启网络服务(service network restart) 

配置完成后虚拟机应该可以上网，宿主机使用xshell等工具连接虚拟机时连接192.168.56.11（eth1的ip），eth0的ip是动态分配的不用管。



## 克隆（复制）虚拟机

<img src="https://irmp.github.io/images/image-20200305124322293.png" alt="image-20200305124322293" style="zoom:70%;" />

选择为所有网卡重新生成MAC地址，下一步选择”完全复制“结束，启动新的虚拟机。

### 网络配置

首先，修改/etc/udev/rules.d/70-persistent-net.rules,

<img src="https://irmp.github.io/images/image-20200305125513402.png" alt="image-20200305125513402" style="zoom: 80%;" />

上面的eth0和eth1是复制过来的，下面的是这次新增的，通过attr{address}对照mac地址可以发现。

我们删除复制过来的，然后留下和新生成的mac地址一致的两个网卡，然后name分别改成eth0和eth1，修改后的文件内容：

```shell
# program, run by the persistent-net-generator.rules rules file.
#
# You can modify it, as long as you keep each rule on a single
# line, and change only the value of the NAME= key.

# PCI device 0x8086:0x100e (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="08:00:27:ae:aa:66", ATTR{type}=="1", KERNEL=="eth*", NAME="eth0"

# PCI device 0x8086:0x100e (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="08:00:27:9c:0e:91", ATTR{type}=="1", KERNEL=="eth*", NAME="eth1"
```

然后需要修改eth1网卡的配置,vi /etc/sysconfig/network-scripts/ifcfg-eth1

```shell
DEVICE=eth1
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=static
DEFROUTE=yes
IPV4_FAILURE_FATAL=yes
IPV6INIT=no
NAME="System eth1"
IPADDR=192.168.56.12 #修改ip地址
HWADDR=08:00:27:9c:0e:91 #修改mac地址
PREFIX=24
UUID=9c92fad9-6ecb-3e6c-eb4d-8a47c6f50c04
LAST_CONNECT=1583377543
```

eth0只需要修改HWADDR，然后修改hostname，vi /etc/sysconfig/network

```shell
NETWORKING=yes
HOSTNAME=hadoop02
NTPSERVERARGS=iburst
```

补一个host文件，vi /etc/hosts

```shell
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.56.11 hadoop01
192.168.56.12 hadoop02
192.168.56.13 hadoop03
```

修改完成后直接reboot，克隆完成。

### 网络配置（增加）

场景：一台主机可以连接另一台主机里的虚拟机，并且虚拟机可以上网，同样双网卡设置，一个NAT不变，另一个改为桥接，网卡配置如下：

```shell
DEVICE=eth1

TYPE=Ethernet

UUID=c31b74b5-3bdc-4077-9a49-4fcc6c1a30c0

ONBOOT=yes

NM_CONTROLLED=yes

BOOTPROTO=static

DEFROUTE=yes

IPV4_FAILURE_FATAL=yes

IPV6INIT=no

NAME="System eth1"

HWADDR=08:00:27:18:b1:00

PEERDNS=yes

PEERROUTES=yes

LAST_CONNECT=1583335310

IPADDR=设置成和主机一个网段

GATEWAY=主机网关

NETMASK=主机子网掩码
```



### 添加mr用户

```shell
useradd mr
passwd mr
su mr
```

sudo vi /etc/sudoers

```shell
## Allow root to run any commands anywhere
root    ALL=(ALL)       ALL
## add mr to sudoers
mr    ALL=(ALL)       ALL
```

创建module和software目录，修改所属用户和组

```shell
cd /opt
sudo mkdir module
sudo mkdir software
sudo chown mr@mr module/ software/
```

```shel
[mr@hadoop01 opt]$ ll
总用量 12
drwxr-xr-x. 2 mr   mr   4096 3月  10 10:26 module
drwxr-xr-x. 2 mr   mr   4096 3月  10 10:23 software
```

### 安装 jdk

upload[^xshell] jdk-8u144-linux-x64.tar.gz to /opt/software/

[^xshell]:https://irmp.github.io/xshell

```shell
tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/
```

检查centos自带jdk，卸载之

```shell
[mr@hadoop01 jdk1.8.0_144]$ rpm -qa|grep java
java-1.5.0-gcj-1.5.0.0-29.1.el6.x86_64
java_cup-0.10k-5.el6.x86_64
gcc-java-4.4.7-23.el6.x86_64
[mr@hadoop01 jdk1.8.0_144]$ sudo yum -y remove java
```

设置环境变量

```shell
sudo vi /etc/profile
#add below to this file
##JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_144
export PATH=$PATH:$JAVA_HOME/bin
```

验证

```shell
source /etc/profile
[mr@hadoop01 jdk1.8.0_144]$ java -version
java version "1.8.0_144"
Java(TM) SE Runtime Environment (build 1.8.0_144-b01)
Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)
```



### 安装hadoop

上传hadoop-2.7.2.tar.gz到/opt/software目录，解压到module目录

```shell
tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/
```

添加环境变量

```shell
sudo vi /etc/profile
#添加下面几行
##HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-2.7.2
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

#最后source让环境变量生效
source /etc/profile
```

验证

```shell
[mr@hadoop01 hadoop-2.7.2]$ hadoop
Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]
  CLASSNAME            run the class named CLASSNAME
 or
  where COMMAND is one of:
  fs                   run a generic filesystem user client
  version              print the version
  jar <jar>            run a jar file
                       note: please use "yarn jar" to launch
                             YARN applications, not this command.
  checknative [-a|-h]  check native hadoop and compression libraries availability
  distcp <srcurl> <desturl> copy file or directories recursively
  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive
  classpath            prints the class path needed to get the
  credential           interact with credential providers
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
  trace                view and modify Hadoop tracing settings

Most commands print help when invoked w/o parameters.
```



## 运行单机版wordcount程序

在hadoop目录下，新建input目录，然后新建输入文件，输入一些单词

```shell
cd /opt/module/hadoop-2.7.2
mkdir input
vi input/wc.input
```

执行hadoop wordcount样例程序

```shell
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount input output
```

查看结果

```shell
[mr@hadoop01 hadoop-2.7.2]$ cat output/part-r-00000 
gaoyang	2
huichao	1
lihua	1
tianyi	1
xiaoheng	1
xinbo	2
yanjing	1
zhangchen	1
```



## 伪分布式搭建

### 修改配置文件

- 修改etc/hadoop/core-site.xml

```xml
<configuration>
    <!-- 指定hdfs中namenode的地址-->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop01:9000</value>
    </property>

    <!-- 指定haoop运行时产生文件的存储目录-->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/module/hadoop-2.7.2/data/tmp</value>
    </property>
</configuration>
```

- 修改etc/hadoop/hadoop-env.sh

```shell
# The only required environment variable is JAVA_HOME.  All others are
# optional.  When running a distributed configuration it is best to
# set JAVA_HOME in this file, so that it is correctly defined on
# remote nodes.
# The java implementation to use.
export JAVA_HOME=/opt/module/jdk1.8.0_144
```

- 修改etc/hadoop/hdfs-site.xml，副本数改成1

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

### 启动集群

- 格式化NameNode

```shel
hdfs namenode -format
```

- 启动NameNode

```shell
[mr@hadoop01 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode
starting namenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-mr-namenode-hadoop01.out
[mr@hadoop01 hadoop-2.7.2]$ jps
2688 Jps
2621 NameNode
```

- 启动DataNode

```shell
[mr@hadoop01 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-mr-datanode-hadoop01.out
[mr@hadoop01 hadoop-2.7.2]$ jps
2711 DataNode
2621 NameNode
2782 Jps
```

- 查看web管理页面

http://hadoop01:50070/

![image-20200311093619057](https://irmp.github.io/images/image-20200311093619057.png)

这里需要关闭centos防火墙或开放50070端口

### 防火墙操作：

```shell
# 查看防火墙状态
service iptables status

# 停止防火墙
service iptables stop

# 启动防火墙
service iptables start

# 重启防火墙
service iptables restart

# 永久关闭防火墙
chkconfig iptables off

# 永久关闭重启
chkconfig iptables on

#开放80端口
vi /etc/sysconfig/iptables
-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT

#保存退出后重启防火墙
service iptables restart
```

### 运行伪分布式版wordcount程序

- hdfs上创建输入目录

```shell
[mr@hadoop01 hadoop-2.7.2]$ hdfs dfs -mkdir -p /user/mr/
```

- 上传本地输入文件

```shell
[mr@hadoop01 hadoop-2.7.2]$ hdfs dfs -put input/ /user/mr/
[mr@hadoop01 hadoop-2.7.2]$ hdfs dfs -lsr /user/mr
lsr: DEPRECATED: Please use 'ls -R' instead.
drwxr-xr-x   - mr supergroup          0 2020-03-11 08:46 /user/mr/input
-rw-r--r--   1 mr supergroup         76 2020-03-11 08:46 /user/mr/input/wc.input
```

- 运行程序,查看结果

```shell
[mr@hadoop01 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/mr/input /user/mr/output

[mr@hadoop01 hadoop-2.7.2]$ hdfs dfs -cat /user/mr/output/p*
gaoyang	2
huichao	1
lihua	1
tianyi	1
xiaoheng	1
xinbo	2
yanjing	1
zhangchen	1
```

### yarn配置

- 修改ect/hadoop/yarn-env.sh

```shell
# some Java parameters
export JAVA_HOME=/opt/module/jdk1.8.0_144
```

- 修改etc/hadoop/yarn-site.xml

```xml
<configuration>
    <!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop01</value>
    </property>
</configuration>
```

- 修改etc/hadoop/mapred-env.sh

```shell
export JAVA_HOME=/opt/module/jdk1.8.0_144
```

- 修改etc/hadoop/yarn-site.xml

```xml
<!-- 指定MR运行在YARN上 -->
<configuration>
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
</configuration>
```

- 启动yarn

```shell
[mr@hadoop01 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager
starting resourcemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-mr-resourcemanager-hadoop01.out
[mr@hadoop01 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager
starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-mr-nodemanager-hadoop01.out
[mr@hadoop01 hadoop-2.7.2]$ jps
2711 DataNode
4520 NodeManager
4552 Jps
4282 ResourceManager
2621 NameNode
```

运行程序

```shell
[mr@hadoop01 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/mr/input /user/mr/output
```

查看8088端口

![image-20200311101146713](https://irmp.github.io/images/image-20200311101146713.png)

#### 配置历史服务器

1.修改etc/hadoop/mapred-site.xml

```xml
<!-- 历史服务器端地址 -->
<property>
	<name>mapreduce.jobhistory.address</name>
    <value>hadoop01:10020</value>
</property>
<!-- 历史服务器web端地址 -->
<property>
	<name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop01:19888</value>
</property>
```

2.启动历史服务器

```shell
[mr@hadoop01 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver
starting historyserver, logging to /opt/module/hadoop-2.7.2/logs/mapred-mr-historyserver-hadoop01.out
[mr@hadoop01 hadoop-2.7.2]$ jps
2711 DataNode
4520 NodeManager
4282 ResourceManager
5101 JobHistoryServer
2621 NameNode
5135 Jps
```

3.查看历史

![image-20200311102541499](https://irmp.github.io/images/image-20200311102541499.png)

#### 配置日志聚集

1.先关掉yarn相关服务

```shell
[mr@hadoop01 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh stop historyserver
stopping historyserver
[mr@hadoop01 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager
stopping nodemanager
nodemanager did not stop gracefully after 5 seconds: killing with kill -9
[mr@hadoop01 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop resourcemanager
stopping resourcemanager
[mr@hadoop01 hadoop-2.7.2]$ jps
5284 Jps
2711 DataNode
2621 NameNode
```

2.修改yarn-site.xml

```xml
<!-- 日志聚集功能 -->
<property>
	<name>yarn.log-aggregation-enable</name>
    <value>true</value>
</property>

<!-- 日志保留时间设置 -->
<property>
	<name>yarn.log-aggregation.retain-seconds</name>
    <value>604800</value>
</property>
```

3.重新启动yarn相关服务，运行wordcount查看logs

```shell
[mr@hadoop01 hadoop-2.7.2]$ sbin/yarn-daemon.sh start resourcemanager
starting resourcemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-mr-resourcemanager-hadoop01.out
[mr@hadoop01 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanager
starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-mr-nodemanager-hadoop01.out
[mr@hadoop01 hadoop-2.7.2]$ sbin/mr-jobhistory-daemon.sh start historyserver
starting historyserver, logging to /opt/module/hadoop-2.7.2/logs/mapred-mr-historyserver-hadoop01.out
[mr@hadoop01 hadoop-2.7.2]$ hdfs dfs -rm -r /user/mr/output
20/03/11 10:41:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /user/mr/output
[mr@hadoop01 hadoop-2.7.2]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/mr/input/ /user/mr/output
```

![image-20200311104336077](https://irmp.github.io/images/image-20200311104336077.png)

**命令行查看yarn日志的方法：**

```shell
yarn logs -applicationId <applicationId>
#mr任务可以直接把job_1583894431103_0001改成application_1583894431103_0001
```

#### 配置文件说明

Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。

（1）默认配置文件：

| 要获取的默认文件     | 文件存放在Hadoop的jar包中的位置                            |
| -------------------- | ---------------------------------------------------------- |
| [core-default.xml]   | hadoop-common-2.7.2.jar/ core-default.xml                  |
| [hdfs-default.xml]   | hadoop-hdfs-2.7.2.jar/ hdfs-default.xml                    |
| [yarn-default.xml]   | hadoop-yarn-common-2.7.2.jar/ yarn-default.xml             |
| [mapred-default.xml] | hadoop-mapreduce-client-core-2.7.2.jar/ mapred-default.xml |

​	（2）自定义配置文件：

​	**core-site.xml**、**hdfs-site.xml**、**yarn-site.xml**、**mapred-site.xml**四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置。

## 完全分布式集群搭建

### 创建集群分发脚本

在/home/mr目录下创建bin目录，并在bin目录下xsync创建文件，文件内容如下

```shell
#!/bin/bash
#1 获取输入参数个数，如果没有参数，直接退出
pcount=$#
if((pcount==0)); then
echo no args;
exit;
fi

#2 获取文件名称
p1=$1
fname=`basename $p1`
echo fname=$fname

#3 获取上级目录到绝对路径
pdir=`cd -P $(dirname $p1); pwd`
echo pdir=$pdir

#4 获取当前用户名称
user=`whoami`

#5 循环
for((host=1; host<4; host++)); do
        echo ------------------- hadoop$host --------------
        rsync -rvl $pdir/$fname $user@hadoop0$host:$pdir
done
```

调用脚本形式：xsync 文件名称

该脚本会将文件和目录同步到其他节点

### 集群部署规划

|      | hadoop01          | hadoop02                    | hadoop03                   |
| ---- | ----------------- | --------------------------- | :------------------------- |
| HDFS | NameNode DataNode | DataNode                    | SecondaryNameNode DataNode |
| YARN | NodeManager       | ResourceManager NodeManager | NodeManager                |

### 配置文件

此处不基于前面的伪分布式，重新给出所有需要修改的文件内容，

进入hadoop01中的/opt/module/hadoop-2.7.2/etc/hadoop目录，

**hdfs-site.xml** 副本数改回3，增加SecondaryNameNode配置

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <!-- 指定Hadoop辅助名称节点主机配置 -->
    <property>
          <name>dfs.namenode.secondary.http-address</name>
          <value>hadoop03:50090</value>
    </property>
</configuration>
```

**yarn-site.xml** 修改resourcemanager地址

```xml
<configuration>
	<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop02</value>
    </property>
    <!-- 日志聚集功能 -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <!-- 日志保留时间设置 -->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
</configuration>
```

**mapred-site.xml** 较伪分布式没有改动

```xml
<configuration>
    <!-- 指定MR运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <!-- 历史服务器端地址 -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>hadoop01:10020</value>
    </property>
    <!-- 历史服务器web端地址 -->
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>hadoop01:19888</value>
    </property>
</configuration>
```

**hadoop-env.sh yarn-env.sh mapred-env.sh**

```shell
export JAVA_HOME=/opt/module/jdk1.8.0_144
```

- 分发文件

```shell
[mr@hadoop01 etc]$ xsync hadoop
fname=hadoop
pdir=/opt/module/hadoop-2.7.2/etc
------------------- hadoop2 --------------
mr@hadoop02's password: 
sending incremental file list
hadoop/core-site.xml
hadoop/hadoop-env.sh
hadoop/hdfs-site.xml
hadoop/mapred-env.sh
hadoop/mapred-site.xml
hadoop/yarn-env.sh
hadoop/yarn-site.xml

sent 5658 bytes  received 272 bytes  1317.78 bytes/sec
total size is 79040  speedup is 13.33
------------------- hadoop3 --------------
mr@hadoop03's password: 
sending incremental file list
hadoop/hdfs-site.xml
hadoop/yarn-site.xml

sent 1723 bytes  received 75 bytes  1198.67 bytes/sec
total size is 79040  speedup is 43.96
```

- 格式化namenode（第一次启动需要）

格式化之前需要先停止hdfs服务，删除hadoop目录下的data和logs目录

```shell
[mr@hadoop01 module]$ rm -rf data/ logs/
[mr@hadoop01 module]$ hdfs namenode -format
```

### 配置SSH免密登陆

hadoop01(NameNode)需要配置mr和root用户的免密登陆

```shell
[mr@hadoop01 .ssh]$ cd 
[mr@hadoop01 ~]$ cd .ssh/
[mr@hadoop01 .ssh]$ ssh-keygen -t rsa
#分发公钥
ssh-copy-id hadoop01
ssh-copy-id hadoop02
ssh-copy-id hadoop03
#切换root用户再来一遍
```

hadoop02(ResourceManager) 需要配置mr用户免密登陆，操作同上

### 启动集群

修改slaves文件，注意不要有空格和空行，最后同步到其他节点

```shell
[mr@hadoop01 hadoop]$ pwd
/opt/module/hadoop-2.7.2/etc/hadoop
[mr@hadoop01 hadoop]$ cat slaves 
hadoop01
hadoop02
hadoop03
[mr@hadoop01 hadoop]$ xsync slaves
fname=slaves
pdir=/opt/module/hadoop-2.7.2/etc/hadoop
------------------- hadoop2 --------------
sending incremental file list
slaves

sent 99 bytes  received 37 bytes  272.00 bytes/sec
total size is 27  speedup is 0.20
------------------- hadoop3 --------------
sending incremental file list
slaves

sent 99 bytes  received 37 bytes  90.67 bytes/sec
total size is 27  speedup is 0.20
```

启动hdfs，在hadoop01（namenode）上通过start-dfs.sh启动

```shell
[mr@hadoop01 hadoop-2.7.2]$ pwd
/opt/module/hadoop-2.7.2
[mr@hadoop01 hadoop-2.7.2]$ jps
2599 Jps
[mr@hadoop01 hadoop-2.7.2]$ sbin/start-dfs.sh
Starting namenodes on [hadoop01]
hadoop01: starting namenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-mr-namenode-hadoop01.out
hadoop03: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-mr-datanode-hadoop03.out
hadoop02: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-mr-datanode-hadoop02.out
hadoop01: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-mr-datanode-hadoop01.out
Starting secondary namenodes [hadoop03]
hadoop03: starting secondarynamenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-mr-secondarynamenode-hadoop03.out
```

启动yarn，在hadoop02（ResourceManager）上通过start-yarn.sh启动

```shell
[mr@hadoop02 hadoop-2.7.2]$ sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-mr-resourcemanager-hadoop02.out
hadoop02: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-mr-nodemanager-hadoop02.out
hadoop01: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-mr-nodemanager-hadoop01.out
hadoop03: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-mr-nodemanager-hadoop03.out
```

最后jps查看各节点服务是否正常：

```shell
[mr@hadoop01 hadoop-2.7.2]$ jps
3090 NodeManager
3187 Jps
2717 NameNode
2830 DataNode

[mr@hadoop02 hadoop-2.7.2]$ jps
2528 DataNode
2645 ResourceManager
2778 Jps
2749 NodeManager

[mr@hadoop03 hadoop-2.7.2]$ jps
2498 DataNode
2787 Jps
2596 SecondaryNameNode
2686 NodeManager
```

### 集群启动停止方式总结

- 各个服务组件逐一启动/停止

| 操作              | 命令                                                         |
| :---------------- | :----------------------------------------------------------- |
| 启动/停止HDFS组件 | hadoop-daemon.sh  start / stop  namenode / datanode / secondarynamenode |
| 启动/停止YARN     | yarn-daemon.sh  start / stop  resourcemanager / nodemanager  |
- 各个模块分开启动/停止（配置ssh是前提）常用

| 操作              | 命令                          |
| ----------------- | ----------------------------- |
| 整体启动/停止HDFS | start-dfs.sh  /  stop-dfs.sh  |
| 整体启动/停止YARN | start-yarn.sh  /  stop-yarn.s |

### 集群时间同步

时间同步的方式：用hadoop01作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。以下所有操作需要root权限

**检查ntp服务是否安装**

```shell
[root@hadoop01 hadoop-2.7.2]# rpm -qa | grep ntp
fontpackages-filesystem-1.41-1.1.el6.noarch
ntp-4.2.6p5-15.el6.centos.x86_64
ntpdate-4.2.6p5-15.el6.centos.x86_64
```

**修改ntp配置文件**

```shell
# Hosts on local network are less restricted.
#这里解注释，根据自己的网段设置ip
restrict 192.168.56.0 mask 255.255.255.0 nomodify notrap

#这里注释掉
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
#添加两行，集群在局域网中，不使用其他互联网上的时间
server 127.127.1.0
fudge 127.127.1.0 stratum 10
```

**修改/etc/sysconfig/ntpd**，让硬件时间与系统时间一起同步，添加如下内容

```shell
SYNC_HWCLOCK=yes
```

**重新启动ntpd服务**

```shell
service ntpd restart
```

**开机启动ntpd服务**

```shell
chkconfig ntpd on
```

**其他机器设置十分钟同步一次**

```shell
[root@hadoop02 hadoop-2.7.2]# crontab -e
#添加如下内容
*/10 * * * * /usr/sbin/ntpdate hadoop01
```

**修改任意机器时间，查看10分钟后是否同步**

```shell
[root@hadoop02 hadoop-2.7.2]# date -s "2018-11-11 11:11:11"
2018年 11月 11日 星期日 11:11:11 CST
[root@hadoop02 hadoop-2.7.2]# date
2018年 11月 11日 星期日 11:11:12 CST
[mr@hadoop02 hadoop-2.7.2]$ date
2020年 03月 12日 星期四 10:02:10 CST
```

